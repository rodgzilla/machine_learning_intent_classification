{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim         as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intent</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>0</td>\n",
       "      <td>I definately would not discuss this with Petro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>1</td>\n",
       "      <td>Check out the most popular, trending, and like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "      <td>1</td>\n",
       "      <td>Let us know how we can help you, your co-worke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>0</td>\n",
       "      <td>The new HomeRun by Deem offer for $10 for Two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>0</td>\n",
       "      <td>• Portland, ME: January 31, 2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Intent                                           Sentence\n",
       "774        0  I definately would not discuss this with Petro...\n",
       "1953       1  Check out the most popular, trending, and like...\n",
       "3440       1  Let us know how we can help you, your co-worke...\n",
       "1575       0  The new HomeRun by Deem offer for $10 for Two ...\n",
       "1937       0                   • Portland, ME: January 31, 2013"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/training_data.csv')\n",
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en      = spacy.load('en')\n",
    "embedding_dim = 100\n",
    "fix_length    = 128\n",
    "epochs        = 20\n",
    "print_every   = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(\n",
    "    sequential = True,\n",
    "    tokenize   = en_tokenizer,\n",
    "    lower      = True,\n",
    "    fix_length = fix_length # for network needing fixed length inputs \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = torchtext.data.Field(\n",
    "    sequential = False,\n",
    "    use_vocab  = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = torchtext.data.TabularDataset.splits(\n",
    "    path        = '../data/',\n",
    "    train       = 'training_data.csv',\n",
    "    test        = 'test_data.csv',\n",
    "    format      = 'csv',\n",
    "    skip_header = True,\n",
    "    fields      = [\n",
    "        ('Intent'  , LABEL),\n",
    "        ('Sentence', TEXT)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = torchtext.data.Iterator.splits(\n",
    "    (train_data, test_data),\n",
    "    sort_key    = lambda x: len(x.Sentence),\n",
    "    batch_sizes = (32, 256),\n",
    "    device      = -1, # -1 for CPU, 0 for GPU\n",
    "    repeat      = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_unk_emb(vocab, init = 'randn', num_special_toks = 2):\n",
    "    emb_vectors = vocab.vectors\n",
    "    sweep_range = len(vocab)\n",
    "    running_norm = 0.\n",
    "    num_non_zero = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    for i in range(num_special_toks, sweep_range):\n",
    "        if len(emb_vectors[i, :].nonzero()) == 0:\n",
    "            if init == 'randn':\n",
    "                torch.nn.init.normal(emb_vectors[i], mean = 0, std = 0.05)\n",
    "        else:\n",
    "            num_non_zero += 1\n",
    "            running_norm += torch.norm(emb_vectors[i])\n",
    "        total_words += 1\n",
    "    print(f'average GloVE norm {running_norm / num_non_zero}, '\n",
    "          f'known words {num_non_zero}, '\n",
    "          f'total words {total_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average GloVE norm 5.217196088951167, known words 6145, total words 6874\n"
     ]
    }
   ],
   "source": [
    "embedding_fn = f'glove.6B.{embedding_dim}d'\n",
    "TEXT.build_vocab(train_data, vectors = embedding_fn)\n",
    "vocab = TEXT.vocab\n",
    "init_unk_emb(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, criterion, train_iter, test_iter):\n",
    "    def compute_loss_and_accuracy(dataset):\n",
    "        batch_number      = len(dataset)\n",
    "        total_loss        = 0\n",
    "        total_predictions = 0\n",
    "        total_correct     = 0\n",
    "        for batch_data in dataset:\n",
    "            X, y               = batch_data.Sentence, batch_data.Intent\n",
    "            y_pred             = model(X)\n",
    "            loss               = criterion(y_pred, y)\n",
    "            total_loss        += loss.data[0]\n",
    "            correct_guesses    = (y_pred.max(dim = 1)[1] == y).sum().data[0]\n",
    "            total_predictions += len(y)\n",
    "            total_correct     += correct_guesses\n",
    "            \n",
    "        accuracy     = total_correct / total_predictions\n",
    "        average_loss = total_loss / batch_number\n",
    "\n",
    "        return accuracy, average_loss\n",
    "            \n",
    "    return compute_loss_and_accuracy(train_iter), \\\n",
    "            compute_loss_and_accuracy(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
    "        self.conv1     = nn.Conv1d(embedding_dim, 32, 3, padding = 1)\n",
    "        self.conv2     = nn.Conv1d(32, 32, 3, padding = 1)\n",
    "        self.conv3     = nn.Conv1d(32, 64, 3, padding = 1)\n",
    "        self.conv4     = nn.Conv1d(64, 64, 3, padding = 1)\n",
    "        self.linear1   = nn.Linear(64 * 32, 2)\n",
    "        \n",
    "        self.embedding.weight.data.copy_(vocab.vectors)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = x.view(-1, 64 * 32)\n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model     = CNN(vocab)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[   30] Running loss:  0.69154\n",
      "\t[   60] Running loss:  0.68119\n",
      "\t[   90] Running loss:  0.66584\n",
      "Epoch 0: train acc  66.667% train loss  0.61131 test acc  77.319% test loss  0.48951\n",
      "\t[   30] Running loss:  0.54985\n",
      "\t[   60] Running loss:  0.51814\n",
      "\t[   90] Running loss:  0.50798\n",
      "Epoch 1: train acc  82.199% train loss  0.40671 test acc  77.621% test loss  0.45421\n",
      "\t[   30] Running loss:  0.38908\n",
      "\t[   60] Running loss:  0.39661\n",
      "\t[   90] Running loss:  0.37190\n",
      "Epoch 2: train acc  89.035% train loss  0.26782 test acc  83.669% test loss  0.36207\n",
      "\t[   30] Running loss:  0.29105\n",
      "\t[   60] Running loss:  0.28615\n",
      "\t[   90] Running loss:  0.27473\n",
      "Epoch 3: train acc  93.465% train loss  0.17577 test acc  81.956% test loss  0.40869\n",
      "\t[   30] Running loss:  0.19449\n",
      "\t[   60] Running loss:  0.17599\n",
      "\t[   90] Running loss:  0.18593\n",
      "Epoch 4: train acc  96.445% train loss  0.09818 test acc  80.746% test loss  0.52517\n",
      "\t[   30] Running loss:  0.10937\n",
      "\t[   60] Running loss:  0.11662\n",
      "\t[   90] Running loss:  0.10987\n",
      "Epoch 5: train acc  98.387% train loss  0.05683 test acc  79.133% test loss  0.62229\n",
      "\t[   30] Running loss:  0.06816\n",
      "\t[   60] Running loss:  0.05982\n",
      "\t[   90] Running loss:  0.05246\n",
      "Epoch 6: train acc  97.430% train loss  0.06987 test acc  81.956% test loss  0.71959\n",
      "\t[   30] Running loss:  0.03661\n",
      "\t[   60] Running loss:  0.04562\n",
      "\t[   90] Running loss:  0.03941\n",
      "Epoch 7: train acc  99.262% train loss  0.02276 test acc  77.621% test loss  0.96548\n",
      "\t[   30] Running loss:  0.02511\n",
      "\t[   60] Running loss:  0.03629\n",
      "\t[   90] Running loss:  0.02155\n",
      "Epoch 8: train acc  99.152% train loss  0.02804 test acc  75.101% test loss  1.23893\n",
      "\t[   30] Running loss:  0.01645\n",
      "\t[   60] Running loss:  0.01082\n",
      "\t[   90] Running loss:  0.01768\n",
      "Epoch 9: train acc  99.781% train loss  0.00852 test acc  75.706% test loss  1.36972\n",
      "\t[   30] Running loss:  0.03634\n",
      "\t[   60] Running loss:  0.02728\n",
      "\t[   90] Running loss:  0.02718\n",
      "Epoch 10: train acc  99.891% train loss  0.00643 test acc  79.435% test loss  1.14651\n",
      "\t[   30] Running loss:  0.00299\n",
      "\t[   60] Running loss:  0.00288\n",
      "\t[   90] Running loss:  0.00953\n",
      "Epoch 11: train acc  98.441% train loss  0.04200 test acc  69.456% test loss  2.30523\n",
      "\t[   30] Running loss:  0.00817\n",
      "\t[   60] Running loss:  0.00859\n",
      "\t[   90] Running loss:  0.00291\n",
      "Epoch 12: train acc  99.809% train loss  0.00747 test acc  79.637% test loss  1.62764\n",
      "\t[   30] Running loss:  0.00263\n",
      "\t[   60] Running loss:  0.00094\n",
      "\t[   90] Running loss:  0.00047\n",
      "Epoch 13: train acc  100.000% train loss  0.00031 test acc  79.335% test loss  1.73977\n",
      "\t[   30] Running loss:  0.00023\n",
      "\t[   60] Running loss:  0.00029\n",
      "\t[   90] Running loss:  0.00025\n",
      "Epoch 14: train acc  100.000% train loss  0.00017 test acc  77.923% test loss  1.87517\n",
      "\t[   30] Running loss:  0.00018\n",
      "\t[   60] Running loss:  0.00017\n",
      "\t[   90] Running loss:  0.00012\n",
      "Epoch 15: train acc  100.000% train loss  0.00012 test acc  78.528% test loss  1.91424\n",
      "\t[   30] Running loss:  0.00010\n",
      "\t[   60] Running loss:  0.00012\n",
      "\t[   90] Running loss:  0.00012\n",
      "Epoch 16: train acc  100.000% train loss  0.00009 test acc  78.327% test loss  1.97523\n",
      "\t[   30] Running loss:  0.00010\n",
      "\t[   60] Running loss:  0.00010\n",
      "\t[   90] Running loss:  0.00007\n",
      "Epoch 17: train acc  100.000% train loss  0.00007 test acc  78.730% test loss  2.00250\n",
      "\t[   30] Running loss:  0.00009\n",
      "\t[   60] Running loss:  0.00005\n",
      "\t[   90] Running loss:  0.00007\n",
      "Epoch 18: train acc  100.000% train loss  0.00006 test acc  78.629% test loss  2.03884\n",
      "\t[   30] Running loss:  0.00007\n",
      "\t[   60] Running loss:  0.00005\n",
      "\t[   90] Running loss:  0.00007\n",
      "Epoch 19: train acc  100.000% train loss  0.00005 test acc  78.730% test loss  2.08644\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, batch_data in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "        X, y          = batch_data.Sentence, batch_data.Intent\n",
    "        y_pred        = model(X)\n",
    "        loss          = criterion(y_pred, y)\n",
    "        running_loss += loss.data[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if i % print_every == print_every - 1:\n",
    "            print(f'\\t[{i + 1:5}] Running loss: {running_loss / print_every : .5f}')\n",
    "            running_loss = 0\n",
    "\n",
    "    (train_acc, train_loss), (test_acc, test_loss) = evaluate_model(\n",
    "        model,\n",
    "        criterion, \n",
    "        train_iter, \n",
    "        test_iter\n",
    "    )\n",
    "    print(f'Epoch {epoch}: train acc {train_acc * 100 : .3f}% '\n",
    "          f'train loss {train_loss : .5f} '\n",
    "          f'test acc {test_acc * 100 : .3f}% '\n",
    "          f'test loss {test_loss : .5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
