{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim         as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intent</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3641</th>\n",
       "      <td>1</td>\n",
       "      <td>You have been requested to provide feedback fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3216</th>\n",
       "      <td>1</td>\n",
       "      <td>Come join me on Popset to create and share bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2488</th>\n",
       "      <td>1</td>\n",
       "      <td>If so, do you want me to meet you there?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>0</td>\n",
       "      <td>FINAL DAY TO SAVE!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>0</td>\n",
       "      <td>This is a system that I've created, tested, tw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Intent                                           Sentence\n",
       "3641       1  You have been requested to provide feedback fo...\n",
       "3216       1  Come join me on Popset to create and share bea...\n",
       "2488       1           If so, do you want me to meet you there?\n",
       "627        0                                 FINAL DAY TO SAVE!\n",
       "1655       0  This is a system that I've created, tested, tw..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/training_data.csv')\n",
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en      = spacy.load('en')\n",
    "embedding_dim = 100\n",
    "fix_length    = 128\n",
    "epochs        = 20\n",
    "print_every   = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(\n",
    "    sequential = True,\n",
    "    tokenize   = en_tokenizer,\n",
    "    lower      = True,\n",
    "    fix_length = fix_length # for network needing fixed length inputs \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = torchtext.data.Field(\n",
    "    sequential = False,\n",
    "    use_vocab  = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = torchtext.data.TabularDataset.splits(\n",
    "    path        = '../data/',\n",
    "    train       = 'training_data.csv',\n",
    "    test        = 'test_data.csv',\n",
    "    format      = 'csv',\n",
    "    skip_header = True,\n",
    "    fields      = [\n",
    "        ('Intent'  , LABEL),\n",
    "        ('Sentence', TEXT)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = torchtext.data.Iterator.splits(\n",
    "    (train_data, test_data),\n",
    "    sort_key    = lambda x: len(x.Sentence),\n",
    "    batch_sizes = (32, 256),\n",
    "    device      = -1, # -1 for CPU, 0 for GPU\n",
    "    repeat      = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_unk_emb(vocab, init = 'randn', num_special_toks = 2):\n",
    "    emb_vectors = vocab.vectors\n",
    "    sweep_range = len(vocab)\n",
    "    running_norm = 0.\n",
    "    num_non_zero = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    for i in range(num_special_toks, sweep_range):\n",
    "        if len(emb_vectors[i, :].nonzero()) == 0:\n",
    "            if init == 'randn':\n",
    "                torch.nn.init.normal(emb_vectors[i], mean = 0, std = 0.05)\n",
    "        else:\n",
    "            num_non_zero += 1\n",
    "            running_norm += torch.norm(emb_vectors[i])\n",
    "        total_words += 1\n",
    "    print(f'average GloVE norm {running_norm / num_non_zero}, '\n",
    "          f'known words {num_non_zero}, '\n",
    "          f'total words {total_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average GloVE norm 5.217196088951167, known words 6145, total words 6874\n"
     ]
    }
   ],
   "source": [
    "embedding_fn = f'glove.6B.{embedding_dim}d'\n",
    "TEXT.build_vocab(train_data, vectors = embedding_fn)\n",
    "vocab = TEXT.vocab\n",
    "init_unk_emb(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, criterion, train_iter, test_iter):\n",
    "    def compute_loss_and_accuracy(dataset):\n",
    "        batch_number      = len(dataset)\n",
    "        total_loss        = 0\n",
    "        total_predictions = 0\n",
    "        total_correct     = 0\n",
    "        for batch_data in dataset:\n",
    "            X, y               = batch_data.Sentence, batch_data.Intent\n",
    "            y_pred             = model(X)\n",
    "            loss               = criterion(y_pred, y)\n",
    "            total_loss        += loss.data[0]\n",
    "            correct_guesses    = (y_pred.max(dim = 1)[1] == y).sum().data[0]\n",
    "            total_predictions += len(y)\n",
    "            total_correct     += correct_guesses\n",
    "            \n",
    "        accuracy     = total_correct / total_predictions\n",
    "        average_loss = total_loss / batch_number\n",
    "\n",
    "        return accuracy, average_loss\n",
    "    \n",
    "    model.eval()\n",
    "    eval_train = compute_loss_and_accuracy(train_iter)\n",
    "    eval_test  = compute_loss_and_accuracy(test_iter)\n",
    "    model.train()\n",
    "    \n",
    "    return eval_train, eval_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
    "        self.conv1     = nn.Conv1d(embedding_dim, 32, 3, padding = 1)\n",
    "        self.conv2     = nn.Conv1d(32, 32, 3, padding = 1)\n",
    "        self.conv3     = nn.Conv1d(32, 64, 3, padding = 1)\n",
    "        self.conv4     = nn.Conv1d(64, 64, 3, padding = 1)\n",
    "        self.linear1   = nn.Linear(64 * 32, 2)\n",
    "        \n",
    "        self.embedding.weight.data.copy_(vocab.vectors)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(F.relu(self.conv1(x)), p = 0.3)\n",
    "        x = F.dropout(F.relu(self.conv2(x)), p = 0.3)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = F.dropout(F.relu(self.conv3(x)), p = 0.1)\n",
    "        x = F.dropout(F.relu(self.conv4(x)), p = 0.1)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = x.view(-1, 64 * 32)\n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model     = CNN(vocab)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[   30] Running loss:  0.68775\n",
      "\t[   60] Running loss:  0.68067\n",
      "\t[   90] Running loss:  0.64949\n",
      "Epoch 0: train acc  72.190% train loss  0.57947 test acc  80.948% test loss  0.54196\n",
      "\t[   30] Running loss:  0.56202\n",
      "\t[   60] Running loss:  0.56433\n",
      "\t[   90] Running loss:  0.47996\n",
      "Epoch 1: train acc  82.882% train loss  0.39979 test acc  83.065% test loss  0.38501\n",
      "\t[   30] Running loss:  0.38146\n",
      "\t[   60] Running loss:  0.41470\n",
      "\t[   90] Running loss:  0.41517\n",
      "Epoch 2: train acc  86.792% train loss  0.30780 test acc  84.577% test loss  0.36608\n",
      "\t[   30] Running loss:  0.29812\n",
      "\t[   60] Running loss:  0.27080\n",
      "\t[   90] Running loss:  0.33583\n",
      "Epoch 3: train acc  91.687% train loss  0.19817 test acc  84.677% test loss  0.35880\n",
      "\t[   30] Running loss:  0.17945\n",
      "\t[   60] Running loss:  0.18836\n",
      "\t[   90] Running loss:  0.19894\n",
      "Epoch 4: train acc  96.336% train loss  0.12658 test acc  78.528% test loss  0.48536\n",
      "\t[   30] Running loss:  0.11813\n",
      "\t[   60] Running loss:  0.15071\n",
      "\t[   90] Running loss:  0.12134\n",
      "Epoch 5: train acc  98.223% train loss  0.06498 test acc  81.552% test loss  0.51397\n",
      "\t[   30] Running loss:  0.06074\n",
      "\t[   60] Running loss:  0.06614\n",
      "\t[   90] Running loss:  0.06737\n",
      "Epoch 6: train acc  99.098% train loss  0.03960 test acc  77.722% test loss  0.68210\n",
      "\t[   30] Running loss:  0.03544\n",
      "\t[   60] Running loss:  0.03452\n",
      "\t[   90] Running loss:  0.05164\n",
      "Epoch 7: train acc  99.672% train loss  0.01818 test acc  80.444% test loss  0.75919\n",
      "\t[   30] Running loss:  0.02399\n",
      "\t[   60] Running loss:  0.03972\n",
      "\t[   90] Running loss:  0.04464\n",
      "Epoch 8: train acc  99.562% train loss  0.01581 test acc  79.536% test loss  0.88363\n",
      "\t[   30] Running loss:  0.02788\n",
      "\t[   60] Running loss:  0.01842\n",
      "\t[   90] Running loss:  0.00760\n",
      "Epoch 9: train acc  99.945% train loss  0.00598 test acc  81.754% test loss  1.05157\n",
      "\t[   30] Running loss:  0.03315\n",
      "\t[   60] Running loss:  0.06905\n",
      "\t[   90] Running loss:  0.02806\n",
      "Epoch 10: train acc  99.863% train loss  0.00842 test acc  81.048% test loss  1.02929\n",
      "\t[   30] Running loss:  0.00377\n",
      "\t[   60] Running loss:  0.00401\n",
      "\t[   90] Running loss:  0.00607\n",
      "Epoch 11: train acc  99.891% train loss  0.00610 test acc  80.343% test loss  1.27154\n",
      "\t[   30] Running loss:  0.01079\n",
      "\t[   60] Running loss:  0.00824\n",
      "\t[   90] Running loss:  0.00309\n",
      "Epoch 12: train acc  100.000% train loss  0.00103 test acc  76.411% test loss  1.70501\n",
      "\t[   30] Running loss:  0.00112\n",
      "\t[   60] Running loss:  0.00042\n",
      "\t[   90] Running loss:  0.00049\n",
      "Epoch 13: train acc  100.000% train loss  0.00029 test acc  77.923% test loss  1.65673\n",
      "\t[   30] Running loss:  0.00024\n",
      "\t[   60] Running loss:  0.00021\n",
      "\t[   90] Running loss:  0.00040\n",
      "Epoch 14: train acc  100.000% train loss  0.00019 test acc  78.125% test loss  1.71625\n",
      "\t[   30] Running loss:  0.00017\n",
      "\t[   60] Running loss:  0.00018\n",
      "\t[   90] Running loss:  0.00022\n",
      "Epoch 15: train acc  100.000% train loss  0.00014 test acc  78.024% test loss  1.79278\n",
      "\t[   30] Running loss:  0.00015\n",
      "\t[   60] Running loss:  0.00017\n",
      "\t[   90] Running loss:  0.00012\n",
      "Epoch 16: train acc  100.000% train loss  0.00011 test acc  78.024% test loss  1.84452\n",
      "\t[   30] Running loss:  0.00010\n",
      "\t[   60] Running loss:  0.00012\n",
      "\t[   90] Running loss:  0.00011\n",
      "Epoch 17: train acc  100.000% train loss  0.00009 test acc  78.125% test loss  1.89948\n",
      "\t[   30] Running loss:  0.00009\n",
      "\t[   60] Running loss:  0.00010\n",
      "\t[   90] Running loss:  0.00009\n",
      "Epoch 18: train acc  100.000% train loss  0.00008 test acc  78.024% test loss  1.96731\n",
      "\t[   30] Running loss:  0.00010\n",
      "\t[   60] Running loss:  0.00006\n",
      "\t[   90] Running loss:  0.00007\n",
      "Epoch 19: train acc  100.000% train loss  0.00006 test acc  78.024% test loss  1.95492\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, batch_data in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "        X, y          = batch_data.Sentence, batch_data.Intent\n",
    "        y_pred        = model(X)\n",
    "        loss          = criterion(y_pred, y)\n",
    "        running_loss += loss.data[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if i % print_every == print_every - 1:\n",
    "            print(f'\\t[{i + 1:5}] Running loss: {running_loss / print_every : .5f}')\n",
    "            running_loss = 0\n",
    "\n",
    "    (train_acc, train_loss), (test_acc, test_loss) = evaluate_model(\n",
    "        model,\n",
    "        criterion, \n",
    "        train_iter, \n",
    "        test_iter\n",
    "    )\n",
    "    print(f'Epoch {epoch}: train acc {train_acc * 100 : .3f}% '\n",
    "          f'train loss {train_loss : .5f} '\n",
    "          f'test acc {test_acc * 100 : .3f}% '\n",
    "          f'test loss {test_loss : .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
