{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim         as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intent</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2596</th>\n",
       "      <td>1</td>\n",
       "      <td>Maybe on Ben's layover we can have a discussio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3543</th>\n",
       "      <td>1</td>\n",
       "      <td>To begin the online administration process, yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0</td>\n",
       "      <td>I am going to get much closer to the model thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3373</th>\n",
       "      <td>1</td>\n",
       "      <td>If you need additional help, visit Apple Support.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3609</th>\n",
       "      <td>1</td>\n",
       "      <td>What would you change about the Senate bill?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Intent                                           Sentence\n",
       "2596       1  Maybe on Ben's layover we can have a discussio...\n",
       "3543       1  To begin the online administration process, yo...\n",
       "242        0  I am going to get much closer to the model thi...\n",
       "3373       1  If you need additional help, visit Apple Support.\n",
       "3609       1       What would you change about the Senate bill?"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/training_data.csv')\n",
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en      = spacy.load('en')\n",
    "embedding_dim = 100\n",
    "fix_length    = 50\n",
    "epochs        = 20\n",
    "print_every   = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(\n",
    "    sequential = True,\n",
    "    tokenize   = en_tokenizer,\n",
    "    lower      = True,\n",
    "    fix_length = fix_length # for network needing fixed length inputs \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = torchtext.data.Field(\n",
    "    sequential = False,\n",
    "    use_vocab  = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = torchtext.data.TabularDataset.splits(\n",
    "    path        = '../data/',\n",
    "    train       = 'training_data.csv',\n",
    "    test        = 'test_data.csv',\n",
    "    format      = 'csv',\n",
    "    skip_header = True,\n",
    "    fields      = [\n",
    "        ('Intent'  , LABEL),\n",
    "        ('Sentence', TEXT)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = torchtext.data.Iterator.splits(\n",
    "    (train_data, test_data),\n",
    "    sort_key    = lambda x: len(x.Sentence),\n",
    "    batch_sizes = (32, 256),\n",
    "    device      = -1, # -1 for CPU, 0 for GPU\n",
    "    repeat      = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_unk_emb(vocab, init = 'randn', num_special_toks = 2):\n",
    "    emb_vectors = vocab.vectors\n",
    "    sweep_range = len(vocab)\n",
    "    running_norm = 0.\n",
    "    num_non_zero = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    for i in range(num_special_toks, sweep_range):\n",
    "        if len(emb_vectors[i, :].nonzero()) == 0:\n",
    "            if init == 'randn':\n",
    "                torch.nn.init.normal(emb_vectors[i], mean = 0, std = 0.05)\n",
    "        else:\n",
    "            num_non_zero += 1\n",
    "            running_norm += torch.norm(emb_vectors[i])\n",
    "        total_words += 1\n",
    "    print(f'average GloVE norm {running_norm / num_non_zero}, '\n",
    "          f'known words {num_non_zero}, '\n",
    "          f'total words {total_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average GloVE norm 5.217196088951167, known words 6145, total words 6874\n"
     ]
    }
   ],
   "source": [
    "embedding_fn = f'glove.6B.{embedding_dim}d'\n",
    "TEXT.build_vocab(train_data, vectors = embedding_fn)\n",
    "vocab = TEXT.vocab\n",
    "init_unk_emb(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, criterion, train_iter, test_iter):\n",
    "    def compute_loss_and_accuracy(dataset):\n",
    "        batch_number      = len(dataset)\n",
    "        total_loss        = 0\n",
    "        total_predictions = 0\n",
    "        total_correct     = 0\n",
    "        for batch_data in dataset:\n",
    "            X, y               = batch_data.Sentence, batch_data.Intent\n",
    "            y_pred             = model(X)\n",
    "            loss               = criterion(y_pred, y)\n",
    "            total_loss        += loss.data[0]\n",
    "            correct_guesses    = (y_pred.max(dim = 1)[1] == y).sum().data[0]\n",
    "            total_predictions += len(y)\n",
    "            total_correct     += correct_guesses\n",
    "            \n",
    "        accuracy     = total_correct / total_predictions\n",
    "        average_loss = total_loss / batch_number\n",
    "\n",
    "        return accuracy, average_loss\n",
    "            \n",
    "    return compute_loss_and_accuracy(train_iter), \\\n",
    "            compute_loss_and_accuracy(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "        super(MLP, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
    "        self.linear1   = nn.Linear(fix_length * embedding_dim, 256)\n",
    "        self.linear2   = nn.Linear(256, 256)\n",
    "        self.linear3   = nn.Linear(256, 2)\n",
    "        \n",
    "        self.embedding.weight.data.copy_(vocab.vectors)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(-1, fix_length * embedding_dim)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model     = MLP(vocab)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[   30] Running loss:  0.66395\n",
      "\t[   60] Running loss:  0.58485\n",
      "\t[   90] Running loss:  0.56589\n",
      "Epoch 0: train acc  73.476% train loss  0.49094 test acc  79.032% test loss  0.46268\n",
      "\t[   30] Running loss:  0.40897\n",
      "\t[   60] Running loss:  0.39595\n",
      "\t[   90] Running loss:  0.39875\n",
      "Epoch 1: train acc  91.660% train loss  0.23547 test acc  78.427% test loss  0.44900\n",
      "\t[   30] Running loss:  0.22364\n",
      "\t[   60] Running loss:  0.24112\n",
      "\t[   90] Running loss:  0.20674\n",
      "Epoch 2: train acc  94.777% train loss  0.15136 test acc  80.141% test loss  0.51741\n",
      "\t[   30] Running loss:  0.12051\n",
      "\t[   60] Running loss:  0.10543\n",
      "\t[   90] Running loss:  0.10840\n",
      "Epoch 3: train acc  98.578% train loss  0.04514 test acc  76.714% test loss  0.84310\n",
      "\t[   30] Running loss:  0.04512\n",
      "\t[   60] Running loss:  0.05547\n",
      "\t[   90] Running loss:  0.08874\n",
      "Epoch 4: train acc  99.617% train loss  0.01760 test acc  76.411% test loss  1.01915\n",
      "\t[   30] Running loss:  0.04770\n",
      "\t[   60] Running loss:  0.02606\n",
      "\t[   90] Running loss:  0.05916\n",
      "Epoch 5: train acc  99.480% train loss  0.02093 test acc  79.133% test loss  0.87565\n",
      "\t[   30] Running loss:  0.01104\n",
      "\t[   60] Running loss:  0.01025\n",
      "\t[   90] Running loss:  0.00714\n",
      "Epoch 6: train acc  99.945% train loss  0.00318 test acc  74.597% test loss  1.47505\n",
      "\t[   30] Running loss:  0.00251\n",
      "\t[   60] Running loss:  0.00173\n",
      "\t[   90] Running loss:  0.00368\n",
      "Epoch 7: train acc  99.945% train loss  0.00227 test acc  72.177% test loss  1.78290\n",
      "\t[   30] Running loss:  0.05973\n",
      "\t[   60] Running loss:  0.02963\n",
      "\t[   90] Running loss:  0.01429\n",
      "Epoch 8: train acc  99.918% train loss  0.00428 test acc  75.302% test loss  1.43518\n",
      "\t[   30] Running loss:  0.01156\n",
      "\t[   60] Running loss:  0.00526\n",
      "\t[   90] Running loss:  0.00390\n",
      "Epoch 9: train acc  99.945% train loss  0.00171 test acc  75.202% test loss  1.74306\n",
      "\t[   30] Running loss:  0.00614\n",
      "\t[   60] Running loss:  0.00710\n",
      "\t[   90] Running loss:  0.00694\n",
      "Epoch 10: train acc  99.699% train loss  0.01025 test acc  74.597% test loss  1.77686\n",
      "\t[   30] Running loss:  0.02114\n",
      "\t[   60] Running loss:  0.01041\n",
      "\t[   90] Running loss:  0.00960\n",
      "Epoch 11: train acc  100.000% train loss  0.00100 test acc  72.681% test loss  1.95195\n",
      "\t[   30] Running loss:  0.01111\n",
      "\t[   60] Running loss:  0.00173\n",
      "\t[   90] Running loss:  0.01964\n",
      "Epoch 12: train acc  99.918% train loss  0.00495 test acc  73.085% test loss  1.87730\n",
      "\t[   30] Running loss:  0.00996\n",
      "\t[   60] Running loss:  0.00327\n",
      "\t[   90] Running loss:  0.00602\n",
      "Epoch 13: train acc  100.000% train loss  0.00083 test acc  75.504% test loss  1.76338\n",
      "\t[   30] Running loss:  0.00333\n",
      "\t[   60] Running loss:  0.00401\n",
      "\t[   90] Running loss:  0.00088\n",
      "Epoch 14: train acc  99.973% train loss  0.00067 test acc  76.815% test loss  1.86525\n",
      "\t[   30] Running loss:  0.00047\n",
      "\t[   60] Running loss:  0.00013\n",
      "\t[   90] Running loss:  0.00187\n",
      "Epoch 15: train acc  100.000% train loss  0.00042 test acc  73.690% test loss  2.29889\n",
      "\t[   30] Running loss:  0.00051\n",
      "\t[   60] Running loss:  0.00013\n",
      "\t[   90] Running loss:  0.00008\n",
      "Epoch 16: train acc  100.000% train loss  0.00006 test acc  75.806% test loss  2.14423\n",
      "\t[   30] Running loss:  0.00006\n",
      "\t[   60] Running loss:  0.00006\n",
      "\t[   90] Running loss:  0.00004\n",
      "Epoch 17: train acc  100.000% train loss  0.00004 test acc  75.706% test loss  2.18342\n",
      "\t[   30] Running loss:  0.00003\n",
      "\t[   60] Running loss:  0.00004\n",
      "\t[   90] Running loss:  0.00003\n",
      "Epoch 18: train acc  100.000% train loss  0.00003 test acc  76.008% test loss  2.20566\n",
      "\t[   30] Running loss:  0.00002\n",
      "\t[   60] Running loss:  0.00003\n",
      "\t[   90] Running loss:  0.00005\n",
      "Epoch 19: train acc  100.000% train loss  0.00002 test acc  76.008% test loss  2.23238\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, batch_data in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "        X, y          = batch_data.Sentence, batch_data.Intent\n",
    "        y_pred        = model(X)\n",
    "        loss          = criterion(y_pred, y)\n",
    "        running_loss += loss.data[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if i % print_every == print_every - 1:\n",
    "            print(f'\\t[{i + 1:5}] Running loss: {running_loss / print_every : .5f}')\n",
    "            running_loss = 0\n",
    "\n",
    "    (train_acc, train_loss), (test_acc, test_loss) = evaluate_model(\n",
    "        model,\n",
    "        criterion, \n",
    "        train_iter, \n",
    "        test_iter\n",
    "    )\n",
    "    print(f'Epoch {epoch}: train acc {train_acc * 100 : .3f}% '\n",
    "          f'train loss {train_loss : .5f} '\n",
    "          f'test acc {test_acc * 100 : .3f}% '\n",
    "          f'test loss {test_loss : .5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
