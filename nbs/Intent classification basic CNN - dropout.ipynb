{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim         as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intent</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>1</td>\n",
       "      <td>Hi: please add $4 of tolls for each day that I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>0</td>\n",
       "      <td>In styles for every decor, at prices to please...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2790</th>\n",
       "      <td>1</td>\n",
       "      <td>Please set the meeting up with Alan, Dan and me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>0</td>\n",
       "      <td>I understand that there may be a need to give ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>1</td>\n",
       "      <td>Do you run a business where you need to send p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Intent                                           Sentence\n",
       "1974       1  Hi: please add $4 of tolls for each day that I...\n",
       "1146       0  In styles for every decor, at prices to please...\n",
       "2790       1   Please set the meeting up with Alan, Dan and me.\n",
       "901        0  I understand that there may be a need to give ...\n",
       "2382       1  Do you run a business where you need to send p..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/training_data.csv')\n",
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en      = spacy.load('en')\n",
    "embedding_dim = 100\n",
    "fix_length    = 128\n",
    "epochs        = 20\n",
    "print_every   = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(\n",
    "    sequential = True,\n",
    "    tokenize   = en_tokenizer,\n",
    "    lower      = True,\n",
    "    fix_length = fix_length # for network needing fixed length inputs \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = torchtext.data.Field(\n",
    "    sequential = False,\n",
    "    use_vocab  = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = torchtext.data.TabularDataset.splits(\n",
    "    path        = '../data/',\n",
    "    train       = 'training_data.csv',\n",
    "    test        = 'test_data.csv',\n",
    "    format      = 'csv',\n",
    "    skip_header = True,\n",
    "    fields      = [\n",
    "        ('Intent'  , LABEL),\n",
    "        ('Sentence', TEXT)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = torchtext.data.Iterator.splits(\n",
    "    (train_data, test_data),\n",
    "    sort_key    = lambda x: len(x.Sentence),\n",
    "    batch_sizes = (32, 256),\n",
    "    device      = -1, # -1 for CPU, 0 for GPU\n",
    "    repeat      = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_unk_emb(vocab, init = 'randn', num_special_toks = 2):\n",
    "    emb_vectors = vocab.vectors\n",
    "    sweep_range = len(vocab)\n",
    "    running_norm = 0.\n",
    "    num_non_zero = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    for i in range(num_special_toks, sweep_range):\n",
    "        if len(emb_vectors[i, :].nonzero()) == 0:\n",
    "            if init == 'randn':\n",
    "                torch.nn.init.normal(emb_vectors[i], mean = 0, std = 0.05)\n",
    "        else:\n",
    "            num_non_zero += 1\n",
    "            running_norm += torch.norm(emb_vectors[i])\n",
    "        total_words += 1\n",
    "    print(f'average GloVE norm {running_norm / num_non_zero}, '\n",
    "          f'known words {num_non_zero}, '\n",
    "          f'total words {total_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average GloVE norm 5.217196088951167, known words 6145, total words 6874\n"
     ]
    }
   ],
   "source": [
    "embedding_fn = f'glove.6B.{embedding_dim}d'\n",
    "TEXT.build_vocab(train_data, vectors = embedding_fn)\n",
    "vocab = TEXT.vocab\n",
    "init_unk_emb(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, criterion, train_iter, test_iter):\n",
    "    def compute_loss_and_accuracy(dataset):\n",
    "        batch_number      = len(dataset)\n",
    "        total_loss        = 0\n",
    "        total_predictions = 0\n",
    "        total_correct     = 0\n",
    "        for batch_data in dataset:\n",
    "            X, y               = batch_data.Sentence, batch_data.Intent\n",
    "            y_pred             = model(X)\n",
    "            loss               = criterion(y_pred, y)\n",
    "            total_loss        += loss.data[0]\n",
    "            correct_guesses    = (y_pred.max(dim = 1)[1] == y).sum().data[0]\n",
    "            total_predictions += len(y)\n",
    "            total_correct     += correct_guesses\n",
    "            \n",
    "        accuracy     = total_correct / total_predictions\n",
    "        average_loss = total_loss / batch_number\n",
    "\n",
    "        return accuracy, average_loss\n",
    "    \n",
    "    model.eval()\n",
    "    eval_train = compute_loss_and_accuracy(train_iter)\n",
    "    eval_test  = compute_loss_and_accuracy(test_iter)\n",
    "    model.train()\n",
    "    \n",
    "    return eval_train, eval_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
    "        self.conv1     = nn.Conv1d(embedding_dim, 32, 3, padding = 1)\n",
    "        self.conv2     = nn.Conv1d(32, 32, 3, padding = 1)\n",
    "        self.conv3     = nn.Conv1d(32, 64, 3, padding = 1)\n",
    "        self.conv4     = nn.Conv1d(64, 64, 3, padding = 1)\n",
    "        self.linear1   = nn.Linear(64 * 32, 2)\n",
    "        \n",
    "        self.embedding.weight.data.copy_(vocab.vectors)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(F.relu(self.conv1(x)), p = 0.7)\n",
    "        x = F.dropout(F.relu(self.conv2(x)), p = 0.7)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = F.dropout(F.relu(self.conv3(x)), p = 0.5)\n",
    "        x = F.dropout(F.relu(self.conv4(x)), p = 0.5)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = x.view(-1, 64 * 32)\n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model     = CNN(vocab)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[   30] Running loss:  0.68909\n",
      "\t[   60] Running loss:  0.66972\n",
      "\t[   90] Running loss:  0.62652\n",
      "Epoch 0: train acc  73.229% train loss  0.54892 test acc  82.258% test loss  0.48735\n",
      "\t[   30] Running loss:  0.50563\n",
      "\t[   60] Running loss:  0.56468\n",
      "\t[   90] Running loss:  0.47055\n",
      "Epoch 1: train acc  83.538% train loss  0.39386 test acc  79.133% test loss  0.45007\n",
      "\t[   30] Running loss:  0.37075\n",
      "\t[   60] Running loss:  0.41177\n",
      "\t[   90] Running loss:  0.37560\n",
      "Epoch 2: train acc  88.050% train loss  0.29005 test acc  80.040% test loss  0.44299\n",
      "\t[   30] Running loss:  0.31796\n",
      "\t[   60] Running loss:  0.29149\n",
      "\t[   90] Running loss:  0.30715\n",
      "Epoch 3: train acc  92.808% train loss  0.19000 test acc  81.048% test loss  0.44214\n",
      "\t[   30] Running loss:  0.19484\n",
      "\t[   60] Running loss:  0.25139\n",
      "\t[   90] Running loss:  0.21588\n",
      "Epoch 4: train acc  93.109% train loss  0.18132 test acc  72.782% test loss  0.64453\n",
      "\t[   30] Running loss:  0.12600\n",
      "\t[   60] Running loss:  0.13545\n",
      "\t[   90] Running loss:  0.13962\n",
      "Epoch 5: train acc  96.254% train loss  0.09811 test acc  82.863% test loss  0.54400\n",
      "\t[   30] Running loss:  0.11908\n",
      "\t[   60] Running loss:  0.06349\n",
      "\t[   90] Running loss:  0.10063\n",
      "Epoch 6: train acc  98.633% train loss  0.05153 test acc  78.528% test loss  0.60112\n",
      "\t[   30] Running loss:  0.08387\n",
      "\t[   60] Running loss:  0.05359\n",
      "\t[   90] Running loss:  0.05295\n",
      "Epoch 7: train acc  99.453% train loss  0.02460 test acc  77.520% test loss  0.90144\n",
      "\t[   30] Running loss:  0.03916\n",
      "\t[   60] Running loss:  0.03481\n",
      "\t[   90] Running loss:  0.04245\n",
      "Epoch 8: train acc  99.562% train loss  0.01777 test acc  78.730% test loss  1.00752\n",
      "\t[   30] Running loss:  0.01424\n",
      "\t[   60] Running loss:  0.01979\n",
      "\t[   90] Running loss:  0.02490\n",
      "Epoch 9: train acc  99.945% train loss  0.00630 test acc  77.823% test loss  1.15484\n",
      "\t[   30] Running loss:  0.00822\n",
      "\t[   60] Running loss:  0.02324\n",
      "\t[   90] Running loss:  0.01583\n",
      "Epoch 10: train acc  99.809% train loss  0.00884 test acc  79.032% test loss  1.20308\n",
      "\t[   30] Running loss:  0.01587\n",
      "\t[   60] Running loss:  0.01310\n",
      "\t[   90] Running loss:  0.01037\n",
      "Epoch 11: train acc  99.918% train loss  0.00643 test acc  76.109% test loss  1.34426\n",
      "\t[   30] Running loss:  0.01281\n",
      "\t[   60] Running loss:  0.00266\n",
      "\t[   90] Running loss:  0.00636\n",
      "Epoch 12: train acc  99.945% train loss  0.00244 test acc  78.024% test loss  1.36376\n",
      "\t[   30] Running loss:  0.00453\n",
      "\t[   60] Running loss:  0.00162\n",
      "\t[   90] Running loss:  0.00075\n",
      "Epoch 13: train acc  99.891% train loss  0.00323 test acc  79.435% test loss  1.46680\n",
      "\t[   30] Running loss:  0.00307\n",
      "\t[   60] Running loss:  0.00624\n",
      "\t[   90] Running loss:  0.00677\n",
      "Epoch 14: train acc  99.918% train loss  0.00231 test acc  79.839% test loss  1.52446\n",
      "\t[   30] Running loss:  0.04202\n",
      "\t[   60] Running loss:  0.01129\n",
      "\t[   90] Running loss:  0.01304\n",
      "Epoch 15: train acc  99.973% train loss  0.00216 test acc  77.923% test loss  1.44444\n",
      "\t[   30] Running loss:  0.00287\n",
      "\t[   60] Running loss:  0.00157\n",
      "\t[   90] Running loss:  0.00576\n",
      "Epoch 16: train acc  99.973% train loss  0.00112 test acc  74.798% test loss  2.08453\n",
      "\t[   30] Running loss:  0.00074\n",
      "\t[   60] Running loss:  0.00451\n",
      "\t[   90] Running loss:  0.00219\n",
      "Epoch 17: train acc  99.945% train loss  0.00199 test acc  78.226% test loss  1.87515\n",
      "\t[   30] Running loss:  0.00020\n",
      "\t[   60] Running loss:  0.01128\n",
      "\t[   90] Running loss:  0.00714\n",
      "Epoch 18: train acc  99.863% train loss  0.00328 test acc  78.730% test loss  1.94115\n",
      "\t[   30] Running loss:  0.00127\n",
      "\t[   60] Running loss:  0.00192\n",
      "\t[   90] Running loss:  0.06378\n",
      "Epoch 19: train acc  98.742% train loss  0.03367 test acc  79.133% test loss  1.16166\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, batch_data in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "        X, y          = batch_data.Sentence, batch_data.Intent\n",
    "        y_pred        = model(X)\n",
    "        loss          = criterion(y_pred, y)\n",
    "        running_loss += loss.data[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if i % print_every == print_every - 1:\n",
    "            print(f'\\t[{i + 1:5}] Running loss: {running_loss / print_every : .5f}')\n",
    "            running_loss = 0\n",
    "\n",
    "    (train_acc, train_loss), (test_acc, test_loss) = evaluate_model(\n",
    "        model,\n",
    "        criterion, \n",
    "        train_iter, \n",
    "        test_iter\n",
    "    )\n",
    "    print(f'Epoch {epoch}: train acc {train_acc * 100 : .3f}% '\n",
    "          f'train loss {train_loss : .5f} '\n",
    "          f'test acc {test_acc * 100 : .3f}% '\n",
    "          f'test loss {test_loss : .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
